"""
Script ƒë·ªÉ fine-tune model sentiment analysis v·ªõi d·ªØ li·ªáu t·ª´ database
"""
import sqlite3
import pandas as pd
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    TrainingArguments, 
    Trainer,
    DataCollatorWithPadding
)
from datasets import Dataset
import torch
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
import os

DB_PATH = 'sentiments.db'
MODEL_NAME = 'wonrax/phobert-base-vietnamese-sentiment'  # PhoBERT cho ti·∫øng Vi·ªát
OUTPUT_DIR = './fine_tuned_model'

def load_training_data():
    """Load v√† chu·∫©n b·ªã d·ªØ li·ªáu t·ª´ database"""
    conn = sqlite3.connect(DB_PATH)
    query = "SELECT text, sentiment FROM sentiments"
    df = pd.read_sql_query(query, conn)
    conn.close()
    
    print(f"ƒê√£ load {len(df)} m·∫´u t·ª´ database")
    
    # Mapping sentiment labels sang numeric
    label_map = {
        'T√çCH C·ª∞C': 2,
        'R·∫§T T√çCH C·ª∞C': 2,
        'TI√äU C·ª∞C': 0,
        'R·∫§T TI√äU C·ª∞C': 0,
        'TRUNG L·∫¨P': 1,
    }
    
    df['label'] = df['sentiment'].map(label_map)
    df = df.dropna(subset=['label'])
    df['label'] = df['label'].astype(int)
    
    print("\nPh√¢n b·ªë nh√£n:")
    print(df['sentiment'].value_counts())
    
    return df

def prepare_dataset(df):
    """Chu·∫©n b·ªã dataset cho training"""
    # Split train/test
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])
    
    # T·∫°o datasets
    train_dataset = Dataset.from_pandas(train_df[['text', 'label']])
    test_dataset = Dataset.from_pandas(test_df[['text', 'label']])
    
    return train_dataset, test_dataset

def tokenize_function(examples, tokenizer):
    """Tokenize text"""
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)

def compute_metrics(eval_pred):
    """T√≠nh metrics cho evaluation"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    
    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='weighted')
    
    return {
        'accuracy': accuracy,
        'f1': f1
    }

def add_manual_training_data(conn):
    """Th√™m d·ªØ li·ªáu training th·ªß c√¥ng cho c√°c tr∆∞·ªùng h·ª£p c∆° b·∫£n"""
    cur = conn.cursor()
    
    # D·ªØ li·ªáu training m·∫´u cho ti·∫øng Vi·ªát
    training_samples = [
        # Ti√™u c·ª±c
        ("t√¥i mu·ªën ch·∫øt", "TI√äU C·ª∞C"),
        ("T√¥i b·ªã ngu", "TI√äU C·ª∞C"),
        ("M√≥n ƒÉn n√†y d·ªü qu√°", "TI√äU C·ª∞C"),
        ("M·ªát m·ªèi qu√°", "TI√äU C·ª∞C"),
        ("T√¥i bu·ªìn v√¨ th·∫•t b·∫°i", "TI√äU C·ª∞C"),
        ("Th·∫≠t t·ªá h·∫°i", "TI√äU C·ª∞C"),
        ("Kh√¥ng th√≠ch c√°i n√†y", "TI√äU C·ª∞C"),
        ("Qu√° t·ªìi t·ªá", "TI√äU C·ª∞C"),
        ("Th·∫•t v·ªçng qu√°", "TI√äU C·ª∞C"),
        ("Ch√°n gh√™", "TI√äU C·ª∞C"),
        
        # T√≠ch c·ª±c
        ("T√¥i r·∫•t vui", "T√çCH C·ª∞C"),
        ("M√≥n n√†y ngon tuy·ªát", "T√çCH C·ª∞C"),
        ("Tuy·ªát v·ªùi qu√°", "T√çCH C·ª∞C"),
        ("T√¥i y√™u ƒëi·ªÅu n√†y", "T√çCH C·ª∞C"),
        ("Qu√° ƒë·ªânh", "T√çCH C·ª∞C"),
        ("Xu·∫•t s·∫Øc", "T√çCH C·ª∞C"),
        ("T√¥i h·∫°nh ph√∫c", "T√çCH C·ª∞C"),
        ("Th·∫≠t tuy·ªát", "T√çCH C·ª∞C"),
        ("T·ªët l·∫Øm", "T√çCH C·ª∞C"),
        ("Ho√†n h·∫£o", "T√çCH C·ª∞C"),
        
        # Trung l·∫≠p
        ("T√¥i l√† ƒê·∫°t", "TRUNG L·∫¨P"),
        ("H√¥m nay th·ª© hai", "TRUNG L·∫¨P"),
        ("C√°i n√†y l√† g√¨", "TRUNG L·∫¨P"),
        ("ƒê∆∞·ª£c ƒë·∫•y", "TRUNG L·∫¨P"),
        ("B√¨nh th∆∞·ªùng", "TRUNG L·∫¨P"),
    ]
    
    from datetime import datetime
    for text, sentiment in training_samples:
        ts = datetime.utcnow().isoformat(sep=' ')
        cur.execute('INSERT INTO sentiments (text, sentiment, timestamp) VALUES (?, ?, ?)', 
                   (text, sentiment, ts))
    
    conn.commit()
    print(f"ƒê√£ th√™m {len(training_samples)} m·∫´u training v√†o database")

def train_model():
    """Main training function"""
    print("=" * 50)
    print("B·∫ÆT ƒê·∫¶U FINE-TUNE MODEL SENTIMENT ANALYSIS")
    print("=" * 50)
    
    # Ki·ªÉm tra v√† th√™m d·ªØ li·ªáu training
    if not os.path.exists(DB_PATH):
        print("‚ùå Database kh√¥ng t·ªìn t·∫°i. Vui l√≤ng ch·∫°y app tr∆∞·ªõc.")
        return
    
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("SELECT COUNT(*) FROM sentiments")
    count = cur.fetchone()[0]
    
    if count < 20:
        print(f"‚ö†Ô∏è  Ch·ªâ c√≥ {count} m·∫´u. ƒêang th√™m d·ªØ li·ªáu training m·∫´u...")
        add_manual_training_data(conn)
    
    conn.close()
    
    # Load data
    print("\nüìä Loading d·ªØ li·ªáu...")
    df = load_training_data()
    
    if len(df) < 10:
        print("‚ùå C·∫ßn √≠t nh·∫•t 10 m·∫´u ƒë·ªÉ train. Vui l√≤ng th√™m nhi·ªÅu d·ªØ li·ªáu h∆°n.")
        return
    
    # Prepare datasets
    print("\nüîß Chu·∫©n b·ªã datasets...")
    train_dataset, test_dataset = prepare_dataset(df)
    
    # Load tokenizer v√† model
    print(f"\nü§ñ Loading model: {MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME,
        num_labels=3,  # 3 labels: ti√™u c·ª±c (0), trung l·∫≠p (1), t√≠ch c·ª±c (2)
        ignore_mismatched_sizes=True
    )
    
    # Tokenize datasets
    print("\n‚úÇÔ∏è  Tokenizing...")
    train_dataset = train_dataset.map(
        lambda x: tokenize_function(x, tokenizer), 
        batched=True
    )
    test_dataset = test_dataset.map(
        lambda x: tokenize_function(x, tokenizer), 
        batched=True
    )
    
    # Data collator
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        learning_rate=2e-5,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        num_train_epochs=3,
        weight_decay=0.01,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        push_to_hub=False,
        logging_steps=10,
    )
    
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )
    
    # Train
    print("\nüöÄ B·∫Øt ƒë·∫ßu training...")
    print("=" * 50)
    trainer.train()
    
    # Evaluate
    print("\nüìà ƒê√°nh gi√° model...")
    results = trainer.evaluate()
    print("\nK·∫øt qu·∫£:")
    for key, value in results.items():
        print(f"  {key}: {value:.4f}")
    
    # Save model
    print(f"\nüíæ L∆∞u model v√†o {OUTPUT_DIR}")
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    print("\n" + "=" * 50)
    print("‚úÖ HO√ÄN TH√ÄNH! Model ƒë√£ ƒë∆∞·ª£c fine-tune v√† l∆∞u th√†nh c√¥ng.")
    print(f"üìÅ Model ƒë∆∞·ª£c l∆∞u t·∫°i: {OUTPUT_DIR}")
    print("\nüí° Kh·ªüi ƒë·ªông l·∫°i app ƒë·ªÉ s·ª≠ d·ª•ng model m·ªõi!")
    print("=" * 50)

if __name__ == "__main__":
    train_model()
